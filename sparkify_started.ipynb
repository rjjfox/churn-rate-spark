{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Sparkify Churn Analysis\n",
    "\n",
    "We want to understand which customers have a high likelihood of downgrading from their premium subscription or cancelling their service altogether.\n",
    "\n",
    "Since we're working with a large dataset of 12GB, we'll utilise Spark clusters to provide the extra computing power."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\r\n",
    "import matplotlib.pyplot as plt\r\n",
    "import seaborn as sns\r\n",
    "from pyspark.sql import SparkSession\r\n",
    "from pyspark.sql.functions import col, when, to_date, to_timestamp, from_unixtime, concat_ws, substring, month, year, count, countDistinct\r\n",
    "from pyspark.sql.functions import sum as Fsum"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create spark session\n",
    "spark = SparkSession \\\n",
    "    .builder \\\n",
    "    .appName(\"Sparkify\") \\\n",
    "    .getOrCreate()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load and Clean Dataset\r\n",
    "\r\n",
    "For much of the process of exploring the data and training the model, we will use a small 128MB subset of the 12GB dataset, before using the full 12GB dataset for the final churn analysis.\r\n",
    "\r\n",
    "The dataset contains rows for unregistered users who are yet to sign into Sparkify. For the purposes of our analysis, we'll remove these rows from our dataset. They're identified by rows containing no session or userId."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "Row(artist='Martha Tilston', auth='Logged In', firstName='Colin', gender='M', itemInSession=50, lastName='Freeman', length=277.89016, level='paid', location='Bakersfield, CA', method='PUT', page='NextSong', registration=1538173362000, sessionId=29, song='Rockpools', status=200, ts=1538352117000, userAgent='Mozilla/5.0 (Windows NT 6.1; WOW64; rv:31.0) Gecko/20100101 Firefox/31.0', userId='30')"
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in full sparkify dataset\n",
    "# event_data = \"s3n://udacity-dsnd/sparkify/sparkify_event_data.json\"\n",
    "\n",
    "# Mini sparkify dataset hosted in an S3 bucket\n",
    "# event_data = \"s3n://udacity-dsnd/sparkify/mini_sparkify_event_data.json\"\n",
    "\n",
    "# Get local file\n",
    "event_data = \"mini_sparkify_event_data.json\"\n",
    "\n",
    "df = spark.read.json(event_data)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove rows without a user or session ID."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.where((col('userId') != '') | (col('sessionId') != '' ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": "278154"
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploratory Data Analysis\r\n",
    "\r\n",
    "We want to understand churn rat. Let's first look at the pages that users can visit. These represent the overall event that occurs.s."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-------------------------+------+\n",
      "|page                     |count |\n",
      "+-------------------------+------+\n",
      "|About                    |495   |\n",
      "|Add Friend               |4277  |\n",
      "|Add to Playlist          |6526  |\n",
      "|Cancel                   |52    |\n",
      "|Cancellation Confirmation|52    |\n",
      "|Downgrade                |2055  |\n",
      "|Error                    |252   |\n",
      "|Help                     |1454  |\n",
      "|Home                     |10082 |\n",
      "|Logout                   |3226  |\n",
      "|NextSong                 |228108|\n",
      "|Roll Advert              |3933  |\n",
      "|Save Settings            |310   |\n",
      "|Settings                 |1514  |\n",
      "|Submit Downgrade         |63    |\n",
      "|Submit Upgrade           |159   |\n",
      "|Thumbs Down              |2546  |\n",
      "|Thumbs Up                |12551 |\n",
      "|Upgrade                  |499   |\n",
      "+-------------------------+------+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('page').count().orderBy('page').show(truncate=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Churn Rate\n",
    "\n",
    "We define churning in this analysis as a user cancelling their subscription and hitting the 'Cancellation Confirmation' page. We make it so that users will only have their cancellation recorded once to give us a binary result."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-----+\n",
      "|churn|count|\n",
      "+-----+-----+\n",
      "|    1|   52|\n",
      "|    0|  173|\n",
      "+-----+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df = df.withColumn(\"Churn\", when(df.page == 'Cancellation Confirmation', 1).otherwise(0))\n",
    "user_df = df.groupBy('userId').agg(when(Fsum('churn')>=1, 1).otherwise(0).alias('churn'))\n",
    "user_df.groupBy('churn').count().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "23.11%\n"
     ]
    }
   ],
   "source": [
    "churn_rate = user_df.select(Fsum('churn')).collect()[0][0]/user_df.select(user_df.churn).count()\n",
    "print(f'{churn_rate:.2%}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# User level features\r\n",
    "\r\n",
    "- Thumbs up/down in last 3 months (+change from month before?)\r\n",
    "- Lifetime as a registered user\r\n",
    "- Number of different artists listened to\r\n",
    "- Time since last active\r\n",
    "- Average number of songs listened to per month\r\n",
    "- Total number of sessions\r\n",
    "\r\n",
    "### Potential Key Features\r\n",
    "\r\n",
    "- Number of friends\r\n",
    "- Number of songs listened to in last month\r\n",
    "- Time spent as paid user\r\n",
    "- Number of adverts\r\n",
    "- Thumbs up/down - counts as an engagement interaction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+------+-----+-----+-------------+------+\n",
      "|userId|churn|songs|uniqueArtists|thumbs|\n",
      "+------+-----+-----+-------------+------+\n",
      "|100010|    0|  275|          252|    22|\n",
      "|200002|    0|  387|          339|    27|\n",
      "|   125|    1|    8|            8|     0|\n",
      "|    51|    1| 2111|         1385|   121|\n",
      "|   124|    0| 4079|         2232|   212|\n",
      "|     7|    0|  150|          142|     8|\n",
      "|    15|    0| 1914|         1302|    95|\n",
      "|    54|    1| 2841|         1744|   192|\n",
      "|   155|    0|  820|          643|    61|\n",
      "|   132|    0| 1928|         1299|   113|\n",
      "|   154|    0|   84|           78|    11|\n",
      "|100014|    1|  257|          233|    20|\n",
      "|   101|    1| 1797|         1241|   102|\n",
      "|    11|    0|  647|          534|    49|\n",
      "|   138|    0| 2070|         1332|   119|\n",
      "|300017|    0| 3632|         2070|   331|\n",
      "|    29|    1| 3028|         1804|   176|\n",
      "|    69|    0| 1125|          865|    81|\n",
      "|100021|    1|  230|          207|    16|\n",
      "|    42|    0| 3573|         2073|   191|\n",
      "+------+-----+-----+-------------+------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "df.groupBy('userId').agg(\r\n",
    "    when(Fsum('churn')>=1, 1).otherwise(0).alias('churn'),\r\n",
    "    count(when(col('Page') == 'NextSong', True)).alias('songs'),\r\n",
    "    countDistinct('artist').alias('uniqueArtists'),\r\n",
    "    count(when(col('Page').isin(['Thumbs Up', 'Thumbs Down']), True)).alias('thumbs'),\r\n",
    ").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Appendix\n",
    "\n",
    "## Downgrades\n",
    "\n",
    "Note how people can downgrade multiple times. Here we see people downgrading up to 3 times. This was not the case for cancellations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+----------+-----+\n",
      "|Downgrades|count|\n",
      "+----------+-----+\n",
      "|         0|  176|\n",
      "|         1|   37|\n",
      "|         3|    2|\n",
      "|         2|   10|\n",
      "+----------+-----+\n",
      "\n"
     ]
    }
   ],
   "source": [
    "dff = df.withColumn(\"Downgrades\", when(df.page == 'Submit Downgrade', 1).otherwise(0))\n",
    "user_dff = dff.groupBy('userId').agg(Fsum('Downgrades').alias('Downgrades'))\n",
    "user_dff.groupBy('Downgrades').count().show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Monthly figures\r\n",
    "\r\n",
    "These were dropped since it was taking a long time to get monthly numbers for a subset of data only lasting 3 months. Figures like this are key for reporting but maybe not for churn analysis.\r\n",
    "\r\n",
    "### Key metrics\r\n",
    "\r\n",
    "- Montly active users over time\r\n",
    "- Daily active users\r\n",
    "- Average listen time per user, incl. partial listens and repeats\r\n",
    "- Total paid vs. unpaid users\r\n",
    "- Total ads served over time"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert the 'ts' column into a datetime feature\r\n",
    "df = df.withColumn(\r\n",
    "    'unix_timestamp',\r\n",
    "    concat_ws(\r\n",
    "        \".\",\r\n",
    "        # Pyspark doesn't handle milliseconds so we divide by 1000\r\n",
    "        from_unixtime((col(\"ts\")/1000), \"yyyy-MM-dd HH:mm:ss\"),\r\n",
    "        substring(col(\"ts\"), -3, 3)\r\n",
    "    )\r\n",
    ")\r\n",
    "\r\n",
    "# Convert this to a daily date feature\r\n",
    "df = df.withColumn('date', to_date(col('unix_timestamp')))\r\n",
    "\r\n",
    "df = df.withColumn('month_year', concat_ws('-', month('date'), year('date')))\r\n",
    "\r\n",
    "# These columns are in case of calculating the number of songs/adverts played per month\r\n",
    "df = df.withColumn(\"NextSong\", when(df.page == 'NextSong', 1).otherwise(0))\r\n",
    "df = df.withColumn(\"RollAdvert\", when(df.page == 'Roll Advert', 1).otherwise(0))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df = df.groupby('userId', 'date').sum().select(\r\n",
    "    col('userId'),\r\n",
    "    col('date'),\r\n",
    "    col('sum(length)').alias('total_length'),\r\n",
    "    col('sum(NextSong)').alias('songs'),\r\n",
    "    col('sum(RollAdvert)').alias('adverts'),\r\n",
    ").toPandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_df.date = pd.to_datetime(daily_df.date)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x.notebook.stdout": "+------+----------+------------------+-----+-----+-------+\n|userId|month_year|      total_length|churn|songs|adverts|\n+------+----------+------------------+-----+-----+-------+\n|   137|   11-2018|27858.652440000005|    0|  113|      7|\n|    95|   11-2018| 92776.29407000003|    0|  374|     26|\n|200015|   10-2018|55171.891479999984|    0|  232|     21|\n+------+----------+------------------+-----+-----+-------+\nonly showing top 3 rows\n\n"
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "monthly_df = df.groupby('userId', 'month_year').sum().select(\r\n",
    "    col('userId'),\r\n",
    "    col('month_year'),\r\n",
    "    col('sum(length)').alias('total_length'),\r\n",
    "    col('sum(churn)').alias('churn'),\r\n",
    "    col('sum(NextSong)').alias('songs'),\r\n",
    "    col('sum(RollAdvert)').alias('adverts'),\r\n",
    ")\r\n",
    "monthly_df.show(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/x.notebook.stdout": "+----------+-----+\n|month_year|count|\n+----------+-----+\n|   11-2018|  187|\n|   10-2018|  213|\n|   12-2018|    4|\n+----------+-----+\n\n"
     },
     "output_type": "unknown"
    }
   ],
   "source": [
    "monthly_df.groupBy('month_year').count().show()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.5  ('venv': venv)",
   "name": "pythonjvsc74a57bd015fb918404e69ee0699fd509b352a8b5e239a015e0f0728dc0e0f951a30526b3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  },
  "metadata": {
   "interpreter": {
    "hash": "15fb918404e69ee0699fd509b352a8b5e239a015e0f0728dc0e0f951a30526b3"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}